{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDycyQGgONqT",
        "outputId": "4caa76a0-fa69-45b7-f269-49fd66335846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pciutils is already the newest version (1:3.7.0-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 CLI\n",
            "############################################################################################# 100.0%\n",
            ">>> Making ollama accessible in the PATH in /usr/local/bin\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 8eeb52dfb3bb... 100% ▕▏ 4.7 GB                         \n",
            "pulling 11ce4ee3e170... 100% ▕▏ 1.7 KB                         \n",
            "pulling 0ba8f0e314b4... 100% ▕▏  12 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 1a4c3c319823... 100% ▕▏  485 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "removing any unused layers \n",
            "success \u001b[?25h\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Create a Python script to start the Ollama API server in a separate thread\n",
        "\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()\n",
        "\n",
        "from IPython.display import clear_output\n",
        "!ollama pull llama3.1:8b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pymilvus openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoQQEFkPQHAs",
        "outputId": "38669b38-5edd-4731-9716-b7fe896c5e9f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymilvus in /usr/local/lib/python3.10/dist-packages (2.4.5)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.42.0)\n",
            "Requirement already satisfied: setuptools>69 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (73.0.1)\n",
            "Requirement already satisfied: grpcio<=1.63.0,>=1.49.1 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (1.63.0)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (3.20.3)\n",
            "Requirement already satisfied: environs<=9.5.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (9.5.0)\n",
            "Requirement already satisfied: ujson>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (5.10.0)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (2.1.4)\n",
            "Requirement already satisfied: milvus-lite<2.5.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (2.4.9)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: marshmallow>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from environs<=9.5.0->pymilvus) (3.22.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from environs<=9.5.0->pymilvus) (1.0.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow>=3.0.0->environs<=9.5.0->pymilvus) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grpcio==1.37.1"
      ],
      "metadata": {
        "id": "Z-MAgssfQ0Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import Collection, connections\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI API client\n",
        "client = OpenAI(api_key='')\n",
        "\n",
        "def generate_embedding(text):\n",
        "    response = client.embeddings.create(\n",
        "        input=text,\n",
        "        model=\"text-embedding-3-small\"\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "def connect_milvus():\n",
        "    connections.connect(host=\"abc.ca-central-1.compute.amazonaws.com\", port=\"19530\")\n",
        "\n",
        "def query_milvus(text):\n",
        "    query_embedding = generate_embedding(text)\n",
        "    collection = Collection(name=\"textbook\")\n",
        "    results = collection.search(\n",
        "        data=[query_embedding],\n",
        "        anns_field=\"embedding\",\n",
        "        param={\"metric_type\": \"L2\"},\n",
        "        limit=1,\n",
        "        output_fields=[\"id\", \"text\"]\n",
        "    )\n",
        "    return results\n",
        "\n",
        "def format_context(results):\n",
        "    context = \"\"\n",
        "    collection = Collection(name=\"textbook\")\n",
        "    for result in results:\n",
        "        for hit in result:\n",
        "            expr = f\"id == {hit.id}\"\n",
        "            entities = collection.query(expr, output_fields=[\"text\"])  # Only retrieve the 'text' field\n",
        "            for entity in entities:\n",
        "                context += entity.get(\"text\", \"\") + \"\\n\"\n",
        "    return context.strip()  # Remove trailing newline if necessary\n"
      ],
      "metadata": {
        "id": "QYIhc68GQYyo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Function to generate a response using the pulled Llama model\n",
        "def generate_response_ollama(prompt):\n",
        "    # The URL of the running Ollama server\n",
        "    ollama_url = \"http://127.0.0.1:11434/v1/completions\"\n",
        "\n",
        "    # Prepare the data for the API request\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    data = {\n",
        "        \"model\": \"llama3.1:8b\",\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": 500,\n",
        "    }\n",
        "\n",
        "    # Send the request to the Ollama API\n",
        "    response = requests.post(ollama_url, headers=headers, data=json.dumps(data))\n",
        "\n",
        "    # Handle the response\n",
        "    if response.status_code == 200:\n",
        "        return response.json()[\"choices\"][0][\"text\"].strip()\n",
        "    else:\n",
        "        return f\"Error: {response.status_code} - {response.text}\"\n",
        "\n",
        "# Example usage of the Ollama API to generate a response\n",
        "connect_milvus()\n",
        "\n",
        "\n",
        "def generate_answer(question):\n",
        "    results = query_milvus(question)\n",
        "    context = format_context(results)\n",
        "    system_prompt = \"You are a knowledgeable and patient AI tutor. Provide clear, detailed, and helpful explanations to the questions asked based on the context provided. provide formulae where necessary\"\n",
        "    prompt = f\"Context: {context} Question: {question}\\nAnswer:\"\n",
        "\n",
        "    # Generate the response using Ollama's API\n",
        "    answer = generate_response_ollama(prompt)\n",
        "    return answer\n",
        "\n"
      ],
      "metadata": {
        "id": "lRum1cx4OrTk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Define stereo vision\"\n",
        "answer= generate_answer(question)\n",
        "print(\"Question:\", question)\n",
        "print(\"Generated Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbPAW99zVRQN",
        "outputId": "c61df690-2b2f-4116-b68f-35abdd053205"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Define stereo vision\n",
            "Generated Answer: Stereo vision refers to how humans perceive solid shapes through the disparity in images captured from different viewpoints. It is a way of seeing 3D structures from 2D images, where the difference in perspective between two or more views allows us to estimate depth information. In essence, it involves taking advantage of the fact that when we look at an object side by side with its reflection or with another view of itself, the difference in appearance (disparity) of features like edges and corners across the images tells us how far away that point is from us.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the principle of stereo vision for 3D shape measurement?\"\n",
        "answer= generate_answer(question)\n",
        "print(\"Question:\", question)\n",
        "print(\"Generated Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHGoYYT2WDuk",
        "outputId": "793a79e7-5700-4da6-9eda-3cd0d39df2e5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the principle of stereo vision for 3D shape measurement?\n",
            "Generated Answer: The principle of stereo vision for 3D shape measurement is based on the idea that when an object is viewed from two different angles, the difference in the position of the object between the two views (known as disparity) is inversely proportional to the distance of the object from the observer. In other words, objects that are farther away appear smaller and have less disparity than objects that are closer to the observer.\n",
            "\n",
            "Specifically, under simple imaging configurations (such as both eyes or cameras looking straight ahead), the disparity between two views is directly related to the depth (distance) of the object in the scene. This fundamental relationship has been utilized in computer vision for various applications such as 3D shape measurement, model building, and image-based rendering.\n",
            "\n",
            "In essence, stereo vision relies on the principles of epipolar geometry, which describes how corresponding points between two images relate to each other in terms of their spatial locations. By establishing dense and accurate inter-image correspondences (i.e., finding the matching pixels or features between two images), stereo matching algorithms can reconstruct a 3D model of the scene. This technique has been widely used in various fields such as photogrammetry, computer vision, robotics, and media production.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are intrinsic and extrinsic camera parameters and how are they obtained through calibration?\"\n",
        "answer= generate_answer(question)\n",
        "print(\"Question:\", question)\n",
        "print(\"Generated Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YTwPaPhWPm4",
        "outputId": "e804dc6e-5bdd-48de-ee39-932a7c3eb7f0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are intrinsic and extrinsic camera parameters and how are they obtained through calibration?\n",
            "Generated Answer: Based on the context, here's an answer to your question:\n",
            "\n",
            "**Intrinsic and Extrinsic Camera Parameters**\n",
            "\n",
            "* **Intrinsic parameters**: These refer to the internal settings of a camera that affect the image it captures. They include:\n",
            "\t+ Focal length (f): distance between the image sensor and the lens\n",
            "\t+ Principal point (cc1, cc2): location of the optical center within the image sensor\n",
            "\t+ Distortion coefficients: parameters that describe radial or tangential distortion in the lenses\n",
            "* **Extrinsic parameters**: These refer to the external position and orientation of a camera. They include:\n",
            "\t+ Translation (tx, ty, tz): movement away from the origin along each axis\n",
            "\t+ Rotation matrix (Rx, Ry, Rz): describes the 3D rotation around each axis\n",
            "\n",
            "These parameters can be estimated through various calibration methods, such as:\n",
            "\n",
            "1. **Camera calibration boards**: Using a known layout of patterns to calculate intrinsic and extrinsic parameters.\n",
            "2. **Photorealistic rendering**: Computer-generated images are rendered with known camera settings to train machine learning models that predict the correct camera parameters.\n",
            "3. **Stereo vision**: Use of two or more cameras looking at the same scene simultaneously to estimate epipolar geometry, which can be used to calculate intrinsic and extrinsic parameters.\n",
            "\n",
            "The specific method described in Chapter 12 uses a set of corresponding points (usually obtained from image matching) to compute the fundamental matrix or essential matrix , which can then be used to establish the epipolar geometry between two images.\n",
            "\n",
            "References:\n",
            "Faugeras, O., & Luong, B. T. D. (2001). The epipolar constraint revisited.\n",
            "Hartley, R. I., & Zisserman, A. (2004). Multiple view geometry in computer vision (2nd ed.). Cambridge University Press.\n",
            "Zhang, Z. (1998a). Determining the camera calibration matrix from two or more views: application of the collinearity assumption.\n",
            "\n",
            "So, intrinsic camera parameters describe the internal settings, while extrinsic parameters describe the external position and orientation of a single camera or relative pose between multiple cameras, as described in this context.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the epipolar constraint and how does help in stereo image matching?\"\n",
        "answer= generate_answer(question)\n",
        "print(\"Question:\", question)\n",
        "print(\"Generated Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPXFk69VWXPa",
        "outputId": "1d919a9c-878b-4d36-c98b-dd71080f8058"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the epipolar constraint and how does help in stereo image matching?\n",
            "Generated Answer: Based on the context provided:\n",
            "\n",
            "The epipolar constraint is that the corresponding point in the unrectified image must lie within the epipolar line, which runs through the image plane parallel to or skew with the projection of the viewing ray at infinity and bounded by the two corresponding epipoles. This constraint helps in stereo image matching by limiting the search for potential correspondences to a specific region in the other image, thereby reducing the number of potential correspondences and increasing the reliability of the matching process.\n",
            "\n",
            "In other words, knowing the epipolar geometry allows us to filter out possibilities early on that an object or point can't possibly correspond to because it lies outside the epipolar line. With this constraint in place, we only need to search for corresponding points along the epipolar line segment corresponding to a given pixel, reducing the search space and speeding up the matching process.\n"
          ]
        }
      ]
    }
  ]
}